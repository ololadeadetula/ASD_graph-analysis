{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013a7357-b40a-48b9-8621-7ec2b6d30e1e",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820dfacc-02b3-4512-ba41-a3a3b4b8fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.load('./AtlasGNNinput/BASC-444_partial correlation_correlation.npy') \n",
    "y = np.load('./AtlasGNNinput/Y.npy')  # Assuming shape: (871,)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f27f3-26c1-4a0b-a57f-ae27d8e757c5",
   "metadata": {},
   "source": [
    "## 2. Graph Arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d077f19-97ca-452c-9b41-6a72d49b9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs, num_nodes, _ = x.shape\n",
    "print(x.shape)\n",
    "num_features = num_nodes * (num_nodes - 1) // 2   \n",
    "\n",
    "print(\"num_graphs:\", num_graphs)   \n",
    "print(\"num_nodes:\", num_nodes)    \n",
    "print(\"num_features:\", num_features)  \n",
    "\n",
    "# List mapping each “feature index” [0..E]  to an (i,j) pair in the upper triangle\n",
    "edge_pairs = [(i, j) for i in range(num_nodes) for j in range(i + 1, num_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe73a4c-b48d-481b-923b-51c776325144",
   "metadata": {},
   "source": [
    "## 3. Parameters for trials and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759244a-73af-480e-a16f-903b0cd96834",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_selected = int(0.20 * num_features)  # 20% of\n",
    "num_trials = 500  # number of repetitions.( I reduced for now due to how long it will take)\n",
    "num_epochs = 30  # number of training epochs per trial. ( reduced for same reason)\n",
    "\n",
    "# Create arrays to store each trial's selected indices and accuracy.\n",
    "# The indices array is shaped as (num_selected, num_trials), where each column represents one trial.\n",
    "indices_matrix = np.zeros((num_selected, num_trials), dtype=int)\n",
    "accuracy_vector = np.zeros(num_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec943d-c85f-4d34-80b8-7aea484595a9",
   "metadata": {},
   "source": [
    "## 4. Generate Graph Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b4248-b783-4a4f-8bb9-e3c72dc51247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def generate_multiple_graphs(X, y):\n",
    "    \"\"\"\n",
    "    Generate multiple PyTorch Geometric graph objects from full symmetric adjacency matrices.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): 3D array of shape (num_graphs, N_nodes, N_nodes) where each entry is an adjacency matrix.\n",
    "        y (numpy.ndarray): 1D array of graph labels.\n",
    "    \n",
    "    Returns:\n",
    "        List[Data]: A list of PyTorch Geometric Data objects.\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    num_graphs = X.shape[0]\n",
    "    N_nodes = X.shape[1] # Assuming square adjacency matrices\n",
    "\n",
    "    for graph_idx in range(num_graphs):\n",
    "        adjacency_matrix = X[graph_idx]\n",
    "        label = y[graph_idx]\n",
    "\n",
    "        # Initialize node features,edge index and weight.\n",
    "        node_features = [[1,] for _ in range(N_nodes)]\n",
    "        edge_index = []\n",
    "        edge_weight = []\n",
    "\n",
    "        # Iterate over upper triangle to avoid self-loops and duplicate edges\n",
    "        for i in range(N_nodes):\n",
    "            for j in range(i + 1, N_nodes): # Avoiding self-loops and duplicate edges\n",
    "                weight = adjacency_matrix[i, j]\n",
    "                if weight != 0: # Only include edges with non-zero weight\n",
    "                    # Add both (i, j) and (j, i) for undirected graph\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(weight) # I removed the additional edge_index since the graph is undirected. \n",
    "                                       \n",
    "                    # Update node features by accumulating connected edge weights\n",
    "                    node_features[i][0] += weight\n",
    "                    node_features[j][0] += weight\n",
    "\n",
    "        # Convert lists to PyTorch tensors\n",
    "        if edge_index:\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "            edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_weight = torch.tensor([], dtype=torch.float)\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_weight=edge_weight, y=label_tensor)\n",
    "        graphs.append(data)\n",
    "                     \n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccd7ba-40a8-48d8-9c41-cbfbe4b2d86a",
   "metadata": {},
   "source": [
    "## 5. GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e161cf5-38fb-458e-8e75-4270669f0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
    "\n",
    "class GraphAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "        super(GraphAttentionModel, self).__init__()\n",
    "        # 1st GATv2 layer: input_dim → hidden_dim, using 'heads' attention heads\n",
    "        # edge_dim=1 since each edge has one feature (FC weight)\n",
    "        self.gat1 = GATv2Conv(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            heads=heads,\n",
    "            dropout=0.3,\n",
    "            edge_dim=1\n",
    "        )\n",
    "        # 2nd GATv2 layer: (hidden_dim * heads) → hidden_dim\n",
    "        self.gat2 = GATv2Conv(\n",
    "            in_channels=hidden_dim * heads,\n",
    "            out_channels=hidden_dim,\n",
    "            heads=heads,\n",
    "            dropout=0.3,\n",
    "            edge_dim=1\n",
    "        )\n",
    "        # 3rd GATv2 layer: (hidden_dim * heads) → output_dim, single head, no concat\n",
    "        self.gat3 = GATv2Conv(\n",
    "            in_channels=hidden_dim * heads,\n",
    "            out_channels=output_dim,\n",
    "            heads=1,\n",
    "            concat=False,\n",
    "            dropout=0.3,\n",
    "            edge_dim=1\n",
    "        )\n",
    "        # Final linear layer for binary classification\n",
    "        self.fc = torch.nn.Linear(output_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, batch):\n",
    "        # Reshape edge_weight from (E,) → (E, 1)\n",
    "        ew = edge_weight.view(-1, 1)\n",
    "\n",
    "        # 1) Apply first GATv2 + ReLU\n",
    "        x = F.relu(self.gat1(x, edge_index, ew))\n",
    "        # 2) Apply second GATv2 + ReLU\n",
    "        x = F.relu(self.gat2(x, edge_index, ew))\n",
    "        # 3) Apply third GATv2 (no activation)\n",
    "        x = self.gat3(x, edge_index, ew)\n",
    "        # 4) Global mean pool over nodes → one graph vector\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # 5) Linear layer + sigmoid for binary output\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd201573-32cc-48d4-ad5d-28c4337e50c2",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67f17c-b148-4aa8-a3de-aac4af52c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    # Train the model for one epoch over the given data loader\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
    "        # Compute loss (reshape output to match labels)\n",
    "        loss = criterion(out.squeeze(1), data.y.float())\n",
    "        # Backward pass and weight update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Return the average loss per batch\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    # Compute overall accuracy of the model on the provided data loader\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            # Forward pass\n",
    "            out = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
    "            # Convert probabilities to binary predictions (threshold = 0.5)\n",
    "            pred = (out.squeeze(1) > 0.5).float()\n",
    "            correct += (pred == data.y).sum().item()\n",
    "            total += data.num_graphs\n",
    "\n",
    "    # Return the fraction of correctly classified graphs\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def eval_loss(model, loader, criterion):\n",
    "    # Compute the average loss of the model over all graphs in the loader\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_graphs = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            # Forward pass\n",
    "            out = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
    "            # Compute loss for this batch, weighted by number of graphs\n",
    "            loss = criterion(out.squeeze(1), data.y.float())\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            total_graphs += data.num_graphs\n",
    "\n",
    "    # Return the average loss per graph\n",
    "    return total_loss / total_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe02e73-0988-4410-a1aa-85cb32387a8d",
   "metadata": {},
   "source": [
    "## 7. Main Loop: Trails of random column removal, train/test split and graph generation/classiication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf41a8-0ab8-43c4-9c13-9e5495ea2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    print(f\"Trial {trial+1}/{num_trials}\")\n",
    "\n",
    "    # 1) Randomly select 20% of the ? upper‐triangle “features” (each maps to one (i,j) pair)\n",
    "    selected_feat_idxs = np.random.choice(num_features, size=num_selected, replace=False)\n",
    "    indices_matrix[:, trial] = selected_feat_idxs  # Save selected indices for this trial\n",
    "\n",
    "    # 2) Create a modified copy of the full (871, E, E) FC matrices\n",
    "    X_mod = x.copy()  # x.shape \n",
    "\n",
    "    #    Zero out each chosen edge (i,j) and its mirror (j,i) across all subjects\n",
    "    for feat_idx in selected_feat_idxs:\n",
    "        i, j = edge_pairs[feat_idx]\n",
    "        X_mod[:, i, j] = 0.0\n",
    "        X_mod[:, j, i] = 0.0\n",
    "\n",
    "    # 3) Generate PyG graphs from the masked FC matrices\n",
    "    graphs = generate_multiple_graphs(X_mod, y)\n",
    "\n",
    "    # 4) Split graphs into train/val/test (80/10/10)\n",
    "    dataset_size = len(graphs)\n",
    "    train_size   = int(0.8 * dataset_size)\n",
    "    val_size     = int(0.1 * dataset_size)\n",
    "    test_size    = dataset_size - train_size - val_size\n",
    "    train_data, val_data, test_data = random_split(graphs, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True)\n",
    "    val_loader   = DataLoader(val_data,   batch_size=32)\n",
    "    test_loader  = DataLoader(test_data,  batch_size=32)\n",
    "\n",
    "    # 5) Initialize model, loss, optimizer, and scheduler \n",
    "    model     = GraphAttentionModel(input_dim=1, hidden_dim=16, output_dim=2)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    # 6) Training loop \n",
    "    best_val_loss    = float('inf')\n",
    "    best_model_state = None\n",
    "    wait             = 0\n",
    "    patience         = 10\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion)\n",
    "        val_loss   = eval_loss(model, val_loader, criterion)\n",
    "        val_acc    = evaluate(model, val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} — Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss    = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            wait             = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"--> Early stopping at epoch {epoch} (no val_loss improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # 7) Test‐set evaluation\n",
    "    test_acc = evaluate(model, test_loader)\n",
    "    accuracy_vector[trial] = test_acc\n",
    "    print(f\"Trial {trial+1} Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c841732-5257-449a-9e83-e7a7a91e7fda",
   "metadata": {},
   "source": [
    "# 8. Saving recall indices/accuracy as npy.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf00688-0cb5-423f-8be8-59cc11d7f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('selected_indices_basc444(500t-part).npy', indices_matrix)# Showcase each column(trial) of indices that were zeroed out. \n",
    "np.save('accuracy_vector_basc444(500t-part).npy', accuracy_vector) # Showcase test accuracy for each trial. 500 trials=500 entries. \n",
    "\n",
    "print(\"Processing complete. The selected indices and accuracy vector have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f744dae-1610-43b4-8786-4b8702f85533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e3d4e64-0f57-4ec5-b050-f41e5f2e6fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy across trials: 0.5396818181818183\n"
     ]
    }
   ],
   "source": [
    "selected_indices = np.load('selected_indices_basc444(500t-part).npy')\n",
    "accuracy_vector = np.load('accuracy_vector_basc444(500t-part).npy')\n",
    "\n",
    "mean_acc = np.mean(accuracy_vector)\n",
    "print(\"Mean accuracy across trials:\", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa63d69-121a-4a9a-b1dc-d242d326dbd8",
   "metadata": {},
   "source": [
    "# 9. Post-hockey Analysis ( Determing Influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee1097-a842-43f2-a318-2c5c014471b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose indices_matrix so each row corresponds to one trial\n",
    "indices_matrix_transposed = indices_matrix.T  # Shape: (num_trials, num_selected)\n",
    "\n",
    "# Compute median accuracy across all trials\n",
    "median_acc = np.median(accuracy_vector)\n",
    "print(\"Median accuracy across trials:\", median_acc)\n",
    "\n",
    "# Split trials into “good” (>= median) and “bad” (< median)\n",
    "good_trials = indices_matrix_transposed[accuracy_vector >= median_acc]\n",
    "bad_trials  = indices_matrix_transposed[accuracy_vector <  median_acc]\n",
    "\n",
    "# Flatten the selected indices for each category\n",
    "good_indices = good_trials.flatten() if good_trials.size > 0 else np.array([])\n",
    "bad_indices  = bad_trials.flatten()  if bad_trials.size  > 0 else np.array([])\n",
    "\n",
    "# Count how often each of the num_features indices was chosen\n",
    "freq_good = np.bincount(good_indices, minlength=num_features)\n",
    "freq_bad  = np.bincount(bad_indices,  minlength=num_features)\n",
    "\n",
    "# Extract the top 50 most frequent indices in each category\n",
    "top_k = 50\n",
    "top_good_idx = np.argsort(freq_good)[-top_k:][::-1]\n",
    "top_bad_idx  = np.argsort(freq_bad)[-top_k:][::-1]\n",
    "\n",
    "print(\"\\nTop 50 indices for good trials and their frequencies:\")\n",
    "for idx in top_good_idx:\n",
    "    print(f\"Index {idx}: {freq_good[idx]} occurrences\")\n",
    "\n",
    "print(\"\\nTop 50 indices for bad trials and their frequencies:\")\n",
    "for idx in top_bad_idx:\n",
    "    print(f\"Index {idx}: {freq_bad[idx]} occurrences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ff70d-4369-4bb9-ab88-9ee376cbf8cc",
   "metadata": {},
   "source": [
    "## 10. Preview Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08fd2b-e36b-483d-acd8-5a068db6ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector = np.load('selected_indices_basc444(500t-part).npy') # when you have required file from custom path\n",
    "print(accuracy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fa325-dac4-4fea-a500-dc6db9c152de",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = np.load('selected_indices_basc444(500t-part).npy') # when you have required file from custom path\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(selected_indices)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2be794-d20c-43d6-a924-802f467c2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nilearn\n",
    "!pip install --upgrade nilearn\n",
    "from nilearn.datasets import fetch_atlas_basc_multiscale_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fd4bbf-4278-43e3-b374-3e5d14cc9b77",
   "metadata": {},
   "source": [
    "## 11. Nilearn Visulization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5710816c-a3b7-4946-acb9-84c5f2ef9456",
   "metadata": {},
   "source": [
    "This is a nilearn visualization using plot.connectome- focus on the top 20 edges from the average of all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fedcc-b981-443b-8747-459fbb545547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.datasets import fetch_atlas_basc_multiscale_2015\n",
    "from nilearn import plotting\n",
    "\n",
    "# Parameters for BASC444\n",
    "num_nodes    = 444\n",
    "num_features = num_nodes * (num_nodes - 1) // 2  \n",
    "top_k        = 20  # how many top edges to show\n",
    "\n",
    "# Load the selected edge indices and accuracy per trial\n",
    "selected_indices = np.load('selected_indices_basc444(500t-part).npy')  # (num_selected, num_trials)\n",
    "accuracy_vector  = np.load('accuracy_vector_basc444(500t-part).npy')   # (num_trials,)\n",
    "\n",
    "# Find all trials where accuracy is below the median (bad trials) as well as the bad indices\n",
    "indices_matrix = selected_indices.T  # shape: (num_trials, num_selected)\n",
    "median_acc     = np.median(accuracy_vector)\n",
    "bad_trials     = indices_matrix[accuracy_vector < median_acc]\n",
    "bad_indices    = bad_trials.flatten() if bad_trials.size > 0 else np.array([], dtype=int)\n",
    "\n",
    "# Count how often each edge appears in bad trials\n",
    "freq_bad = np.bincount(bad_indices, minlength=num_features)\n",
    "\n",
    "# Pick the top_k edges that appear most often in bad trials\n",
    "top_bad_idx = np.argsort(freq_bad)[-top_k:][::-1]\n",
    "\n",
    "# Helper to convert a flat edge index k to (i,j) in a n*n matrix\n",
    "def index_to_coords(k, n=num_nodes):\n",
    "    total = 0\n",
    "    for i in range(n - 1):\n",
    "        count = n - i - 1\n",
    "        if k < total + count:\n",
    "            j = i + 1 + (k - total)\n",
    "            return i, j\n",
    "        total += count\n",
    "    raise ValueError(\"Index out of range.\")\n",
    "\n",
    "# Build a symmetric “frequency” matrix for those critical edges\n",
    "matrix_critical = np.zeros((num_nodes, num_nodes))\n",
    "for idx in top_bad_idx:\n",
    "    i, j = index_to_coords(int(idx))\n",
    "    matrix_critical[i, j] = freq_bad[idx]\n",
    "    matrix_critical[j, i] = freq_bad[idx]\n",
    "\n",
    "# Fetch AAL centroids for plotting\n",
    "basc = fetch_atlas_basc_multiscale_2015()           \n",
    "coords = plotting.find_parcellation_cut_coords(basc.scale444)  \n",
    "\n",
    "# Plot only those critical connections on a glass brain\n",
    "threshold = freq_bad[top_bad_idx[-1]]  # lowest frequency among the chosen edges\n",
    "display = plotting.plot_connectome(\n",
    "    matrix_critical,\n",
    "    coords,\n",
    "    edge_threshold=threshold,\n",
    "    node_size=10,\n",
    "    title=f\"Top {top_k} Critical Connections (Bad Trials) — BASC444\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42d34d12-ac28-4ad4-acfd-047a5c1e7e0c",
   "metadata": {},
   "source": [
    "This is a nilearn visualization using plot.connectome- focus on the top 20 edges from the average of all trials (Better visualiztion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72607c2-8360-4016-8a71-bdf6b7f29546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.datasets import fetch_atlas_basc_multiscale_2015\n",
    "from nilearn import plotting\n",
    "\n",
    "# Parameters for BASC444\n",
    "num_nodes    = 444\n",
    "num_features = num_nodes * (num_nodes - 1) // 2  \n",
    "top_k        = 20      # edges per category\n",
    "\n",
    "# Load trial data\n",
    "selected_indices = np.load('selected_indices_basc444(500t-part).npy')  # shape: (num_selected, num_trials)\n",
    "accuracy_vector  = np.load('accuracy_vector_basc444(500t-part).npy')   # shape: (num_trials,)\n",
    "\n",
    "# Prepare split by median accuracy\n",
    "indices_matrix = selected_indices.T                 # (num_trials, num_selected)\n",
    "median_acc     = np.median(accuracy_vector)\n",
    "\n",
    "# Bad trials / critical edges\n",
    "bad_trials     = accuracy_vector < median_acc\n",
    "bad_indices  = indices_matrix[bad_trials].flatten() if bad_trials.any() else np.array([], dtype=int)\n",
    "freq_bad     = np.bincount(bad_indices, minlength=num_features)\n",
    "top_bad_idx  = np.argsort(freq_bad)[-top_k:][::-1]\n",
    "\n",
    "# Good trials / redundant edges\n",
    "good_trials     = accuracy_vector >= median_acc\n",
    "good_indices  = indices_matrix[good_trials].flatten() if good_trials.any() else np.array([], dtype=int)\n",
    "freq_good     = np.bincount(good_indices, minlength=num_features)\n",
    "top_good_idx  = np.argsort(freq_good)[-top_k:][::-1]\n",
    "\n",
    "# Index→(i,j) helper for upper‐triangle of an n×n matrix\n",
    "def index_to_coords(k, n=num_nodes):\n",
    "    tot = 0\n",
    "    for i in range(n - 1):\n",
    "        cnt = n - i - 1\n",
    "        if k < tot + cnt:\n",
    "            return i, i + 1 + (k - tot)\n",
    "        tot += cnt\n",
    "    raise ValueError(f\"Index {k} out of range\")\n",
    "\n",
    "# Plotting helper\n",
    "def plot_edges(edge_idx_list, freq_arr, title):\n",
    "    # build full symmetric weight matrix\n",
    "    M = np.zeros((num_nodes, num_nodes))\n",
    "    connected = np.zeros(num_nodes, bool)\n",
    "    for k in edge_idx_list:\n",
    "        i, j = index_to_coords(int(k))\n",
    "        M[i, j] = freq_arr[k]\n",
    "        M[j, i] = freq_arr[k]\n",
    "        connected[[i, j]] = True\n",
    "\n",
    "    # node styling\n",
    "    node_sizes  = [20 if c else 10 for c in connected]\n",
    "    node_colors = ['blue' if c else 'darkgray' for c in connected]\n",
    "\n",
    "    # fetch BASC444 centroids\n",
    "    basc = fetch_atlas_basc_multiscale_2015()\n",
    "    coords = plotting.find_parcellation_cut_coords(basc.scale444)\n",
    "\n",
    "    # edge threshold = smallest of the top‐K freqs\n",
    "    thresh = float(freq_arr[edge_idx_list[-1]])\n",
    "    plotting.plot_connectome(\n",
    "        M, coords,\n",
    "        edge_threshold=thresh,\n",
    "        node_size=node_sizes,\n",
    "        node_color=node_colors,\n",
    "        title=title\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Visualize both panels\n",
    "plot_edges(top_bad_idx,  freq_bad,  f\"Top {top_k} Critical Connections (Bad Trials)\")\n",
    "plot_edges(top_good_idx, freq_good, f\"Top {top_k} Redundant Connections (Good Trials)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2878e03f-92c6-458a-ab76-191a23ddbdf6",
   "metadata": {},
   "source": [
    "This is a nilearn visualization using plot.anat- focus on the top 20 edges from the average of all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a0e7-eea8-465f-815b-1f844e4f8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.datasets import fetch_atlas_basc_multiscale_2015\n",
    "from nilearn import plotting\n",
    "\n",
    "# Parameters for BASC444\n",
    "num_nodes    = 444\n",
    "num_features = num_nodes * (num_nodes - 1) // 2  \n",
    "top_k        = 20\n",
    "\n",
    "# Load trial data\n",
    "selected_indices = np.load('selected_indices_basc444(500t-part).npy')  # (num_selected, num_trials)\n",
    "accuracy_vector  = np.load('accuracy_vector_basc444(500t-part).npy')   # (num_trials,)\n",
    "\n",
    "# Prepare median split\n",
    "indices_matrix = selected_indices.T                 # (num_trials, num_selected)\n",
    "median_acc     = np.median(accuracy_vector)\n",
    "\n",
    "bad_trials  = indices_matrix[accuracy_vector < median_acc]\n",
    "bad_indices = bad_trials.flatten() if bad_trials.size > 0 else np.array([], dtype=int)\n",
    "freq_bad    = np.bincount(bad_indices, minlength=num_features)\n",
    "top_bad_idx = np.argsort(freq_bad)[-top_k:][::-1]\n",
    "\n",
    "good_trials  = indices_matrix[accuracy_vector >= median_acc]\n",
    "good_indices = good_trials.flatten() if good_trials.size > 0 else np.array([], dtype=int)\n",
    "freq_good    = np.bincount(good_indices, minlength=num_features)\n",
    "top_good_idx = np.argsort(freq_good)[-top_k:][::-1]\n",
    "\n",
    "#  Map 1D edge-index → (i, j) in the n×n matrix \n",
    "def index_to_coords(k, n=num_nodes):\n",
    "    total = 0\n",
    "    for i in range(n - 1):\n",
    "        count = n - i - 1\n",
    "        if k < total + count:\n",
    "            return i, i + 1 + (k - total)\n",
    "        total += count\n",
    "    raise ValueError(f\"Index {k} out of range for n={n}.\")\n",
    "\n",
    "# Plot nodes on ortho slices centered on their centroid\n",
    "def plot_nodes_ortho(edge_idx_list, title):\n",
    "    # 1. Flag which nodes are involved\n",
    "    connected = np.zeros(num_nodes, dtype=bool)\n",
    "    for idx in edge_idx_list:\n",
    "        i, j = index_to_coords(int(idx))\n",
    "        connected[i] = True\n",
    "        connected[j] = True\n",
    "\n",
    "    # 2. Fetch BASC444 centroids\n",
    "    basc   = fetch_atlas_basc_multiscale_2015()\n",
    "    coords = plotting.find_parcellation_cut_coords(basc.scale444)\n",
    "\n",
    "    # 3. Extract only the active-node coords\n",
    "    coords_active = np.array([coords[i] for i, flag in enumerate(connected) if flag])\n",
    "    if coords_active.size == 0:\n",
    "        print(f\"No nodes to plot for: {title}\")\n",
    "        return\n",
    "\n",
    "    # 4. Compute mean center\n",
    "    center = tuple(coords_active.mean(axis=0))\n",
    "\n",
    "    # 5. Plot the anatomical slices\n",
    "    display = plotting.plot_anat(\n",
    "        title=title,\n",
    "        display_mode='ortho',\n",
    "        cut_coords=center\n",
    "    )\n",
    "\n",
    "    # 6. Overlay markers at each active centroid\n",
    "    display.add_markers(\n",
    "        coords_active.tolist(),\n",
    "        marker_color='blue',\n",
    "        marker_size=50\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Visualize critical vs. redundant nodes\n",
    "plot_nodes_ortho(top_bad_idx,  f\"Top {top_k} Critical Nodes (Bad Trials)\")\n",
    "plot_nodes_ortho(top_good_idx, f\"Top {top_k} Redundant Nodes (Good Trials)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88e38240-9714-4cf6-8d22-426ffeb4c510",
   "metadata": {},
   "source": [
    "This is a nilearn visualization using plot.connectome- focus on the top 20 edges from the worst trial and the best trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553cb80-8bba-44d1-8004-f97c82488717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.datasets import fetch_atlas_basc_multiscale_2015\n",
    "from nilearn import plotting\n",
    "\n",
    "# Load your trial data \n",
    "selected_indices = np.load('selected_indices_basc444(500t-part).npy')  # shape: (num_selected, num_trials)\n",
    "accuracy_vector  = np.load('accuracy_vector_basc444(500t-part).npy')   # shape: (num_trials,)\n",
    "\n",
    "# Parameters for BASC444\n",
    "num_nodes    = 444\n",
    "num_features = num_nodes * (num_nodes - 1) // 2  \n",
    "top_k        = 20\n",
    "\n",
    "#  Split by median accuracy\n",
    "median_acc = np.median(accuracy_vector)\n",
    "all_trials = np.arange(len(accuracy_vector))\n",
    "\n",
    "bad_trials  = accuracy_vector < median_acc\n",
    "good_trials = accuracy_vector >= median_acc\n",
    "\n",
    "# Identify the single worst (lowest‐acc) bad trial and best (highest‐acc) good trial\n",
    "bad_trials = all_trials[bad_trials]\n",
    "best_bad   = bad_trials[np.argmin(accuracy_vector[bad_trials])] if bad_trials.size else None\n",
    "\n",
    "good_trials = all_trials[good_trials]\n",
    "best_good   = good_trials[np.argmax(accuracy_vector[good_trials])] if good_trials.size else None\n",
    "\n",
    "# Precompute global edge frequencies over bad vs. good sets\n",
    "indices_matrix = selected_indices.T  # (num_trials, num_selected)\n",
    "freq_bad  = np.bincount(indices_matrix[bad_trials].flatten(),  minlength=num_features)\n",
    "freq_good = np.bincount(indices_matrix[good_trials].flatten(), minlength=num_features)\n",
    "\n",
    "# Helper: map 1D edge‐index → (i,j) in the upper triangle of an n×n matrix\n",
    "def index_to_coords(k, n=num_nodes):\n",
    "    total = 0\n",
    "    for i in range(n - 1):\n",
    "        cnt = n - i - 1\n",
    "        if k < total + cnt:\n",
    "            return i, i + 1 + (k - total)\n",
    "        total += cnt\n",
    "    raise ValueError(f\"Index {k} out of range for n={n}\")\n",
    "\n",
    "# Fetch BASC444 centroids once\n",
    "basc    = fetch_atlas_basc_multiscale_2015()\n",
    "coords  = plotting.find_parcellation_cut_coords(basc.scale444)\n",
    "\n",
    "def plot_top20(trial_idx, freq_array, label):\n",
    "    if trial_idx is None:\n",
    "        print(f\"No {label} trials to plot.\")\n",
    "        return\n",
    "\n",
    "    acc    = accuracy_vector[trial_idx]\n",
    "    masked = selected_indices[:, trial_idx]   # which edges were zeroed in that trial\n",
    "\n",
    "    # rank those edges by their overall freq, take top_k\n",
    "    ranked = masked[np.argsort(freq_array[masked])[::-1]]\n",
    "    top20   = ranked[:top_k]\n",
    "\n",
    "    # build n×n adjacency with only those edges\n",
    "    M = np.zeros((num_nodes, num_nodes))\n",
    "    involved = np.zeros(num_nodes, bool)\n",
    "    for k in top20:\n",
    "        i, j = index_to_coords(int(k))\n",
    "        M[i, j] = freq_array[int(k)]\n",
    "        M[j, i] = freq_array[int(k)]\n",
    "        involved[i] = True\n",
    "        involved[j] = True\n",
    "\n",
    "    # style nodes\n",
    "    sizes  = [30 if involved[n] else 10 for n in range(num_nodes)]\n",
    "    colors = ['blue' if involved[n] else 'darkgray' for n in range(num_nodes)]\n",
    "\n",
    "    plotting.plot_connectome(\n",
    "        M,\n",
    "        coords,\n",
    "        edge_threshold=min(freq_array[top20]),\n",
    "        node_size=sizes,\n",
    "        node_color=colors,\n",
    "        title=f\"{label} Trial #{trial_idx} — Acc={acc:.3f}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Plot the two panels\n",
    "plot_top20(best_bad,  freq_bad,  \"Worst-Bad (Critical Edges)\")\n",
    "plot_top20(best_good, freq_good, \"Best-Good (Non-critical Edges)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7832b34c-e09d-41fe-b496-1bd31baaed14",
   "metadata": {},
   "source": [
    "This is a nilearn visualization using plot.anat- focus on the top 20 edges from the worst trial and the best trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb232f-f5eb-4182-90f1-f813c979e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.datasets import fetch_atlas_basc_multiscale_2015\n",
    "from nilearn import plotting\n",
    "\n",
    "# Load your trial data\n",
    "selected_indices = np.load('selected_indices_basc444(500t-part).npy')  # (num_selected, num_trials)\n",
    "accuracy_vector  = np.load('accuracy_vector_basc444(500t-part).npy')   # (num_trials,)\n",
    "\n",
    "# Parameters for BASC444\n",
    "num_nodes    = 444\n",
    "num_features = num_nodes * (num_nodes - 1) // 2   \n",
    "top_k        = 20\n",
    "\n",
    "# Split by median accuracy\n",
    "median_acc = np.median(accuracy_vector)\n",
    "all_trials = np.arange(len(accuracy_vector))\n",
    "\n",
    "bad_trials  = accuracy_vector < median_acc\n",
    "good_trials = accuracy_vector >= median_acc\n",
    "\n",
    "bad_trials  = all_trials[bad_trials]\n",
    "best_bad    = bad_trials[np.argmin(accuracy_vector[bad_trials])] \\\n",
    "                if bad_trials.size else None\n",
    "\n",
    "good_trials = all_trials[good_trials]\n",
    "best_good   = good_trials[np.argmax(accuracy_vector[good_trials])] \\\n",
    "                if good_trials.size else None\n",
    "\n",
    "# Global edge‐frequency maps\n",
    "indices_matrix = selected_indices.T  # (num_trials, num_selected)\n",
    "freq_bad  = np.bincount(indices_matrix[bad_trials].flatten(),  minlength=num_features)\n",
    "freq_good = np.bincount(indices_matrix[good_trials].flatten(), minlength=num_features)\n",
    "\n",
    "#  Helper: map 1D edge‐index → (i,j) in n×n upper triangle\n",
    "def index_to_coords(k, n=num_nodes):\n",
    "    total = 0\n",
    "    for i in range(n - 1):\n",
    "        cnt = n - i - 1\n",
    "        if k < total + cnt:\n",
    "            return i, i + 1 + (k - total)\n",
    "        total += cnt\n",
    "    raise ValueError(f\"Index {k} out of range for n={n}\")\n",
    "\n",
    "# Ortho‐slice plotting for the top-5 edges of one trial\n",
    "def plot_top20_ortho(trial_idx, freq_array, label):\n",
    "    if trial_idx is None:\n",
    "        print(f\"No {label} trials to plot.\")\n",
    "        return\n",
    "    acc    = accuracy_vector[trial_idx]\n",
    "    masked = selected_indices[:, trial_idx]  # edges zeroed in this trial\n",
    "\n",
    "    # rank this trial’s edges by their global freq, take top_k\n",
    "    ranked = masked[np.argsort(freq_array[masked])[::-1]]\n",
    "    top20   = ranked[:top_k]\n",
    "\n",
    "    # mark which nodes get involved\n",
    "    involved = np.zeros(num_nodes, dtype=bool)\n",
    "    for k in top20:\n",
    "        i, j = index_to_coords(int(k))\n",
    "        involved[i] = True\n",
    "        involved[j] = True\n",
    "\n",
    "    # fetch BASC197 centroids\n",
    "    basc   = fetch_atlas_basc_multiscale_2015()\n",
    "    coords = plotting.find_parcellation_cut_coords(basc.scale444)\n",
    "\n",
    "    # select only the active-node coords\n",
    "    coords_active = [coords[i] for i, flag in enumerate(involved) if flag]\n",
    "    if not coords_active:\n",
    "        print(f\"No nodes to plot for {label} Trial #{trial_idx}\")\n",
    "        return\n",
    "\n",
    "    # center the ortho view\n",
    "    center = tuple(np.mean(coords_active, axis=0))\n",
    "\n",
    "    # show the anatomy + markers\n",
    "    display = plotting.plot_anat(\n",
    "        title=f\"{label} Trial #{trial_idx} — Acc={acc:.3f}\",\n",
    "        display_mode='ortho',\n",
    "        cut_coords=center\n",
    "    )\n",
    "    display.add_markers(\n",
    "        coords_active,\n",
    "        marker_color='blue',\n",
    "        marker_size=50\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Render both panels\n",
    "plot_top20_ortho(best_bad,  freq_bad,  \"Worst-Bad (Critical Location)\")\n",
    "plot_top20_ortho(best_good, freq_good, \"Best-Good (Non-critical Location)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf619d5-4596-49b8-b71c-81f75376293e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
